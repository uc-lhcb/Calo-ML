{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iedified (new)\n",
    "from torchsummary import summary\n",
    "# need this one to save print output\n",
    "import sys\n",
    "\n",
    "# original\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from models import modelSelector\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import sys\n",
    "\n",
    "from timeit import default_timer as timer #Calculate time in GPU and CPU.\n",
    "import torch\n",
    "import torch.nn as nn # torch.nn gives us access to some helpful neural network things, such as fully-connected layers, convolutional layers (for imagery), recurrent layers, ...\n",
    "import torch.nn.functional as F # handy functions like RELu (activation functions).\n",
    "from sklearn.model_selection import train_test_split #The name is self explainatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(enThreshold, KFold, model, variant, learning_rate, BATCH_SIZE_Factor, EPOCHS, loss_function, optimizer):\n",
    "    '''\n",
    "    # enThreshold: Many cells have very small values of desposited energy. Replace values below a certain threshold with zero.\n",
    "    # KFold: ratio of train and test. Example: KFold of \"5\" would be train 80% and test 20%, KFold of \"2\" would be train and test 50%.\n",
    "    # model: number of net chosen from the file \"models.py\".\n",
    "    # variant: variant of the net chosen from the file \"models.py\".\n",
    "    # learning_rate: learning rate for optimization function.\n",
    "    # BATCH_SIZE_Factor: it is just prepared, but does not work in this version.\n",
    "    # EPOCHS: Number of passes through all the dataset.\n",
    "    # loss_function: 0 is CrossEntropyLoss and 1 NLLLoss, but you can edit the code to add as many as you want.\n",
    "    # optimizer: 0 is Adam, 1 is SGD and 2 is Adamax, but you can edit the code to add as many as you want.    \n",
    "    '''    \n",
    "    spacal_df = pd.read_hdf('spacal_at10mm_neutrals.h5') #ToDo: Write the path where you store the hdf files.\n",
    "\n",
    "    #Change ids for integrity\n",
    "    spacal_df['class'] = spacal_df['class'].map(lambda x: np.where(x == 2, 0, x))\n",
    "    spacal_df['class'] = spacal_df['class'].map(lambda x: np.where(x == 10, 1, x))\n",
    "\n",
    "    particleNum = 2\n",
    "\n",
    "\n",
    "    #Replacement with threshold value:\n",
    "    spacal_df['EnergyFront'] = spacal_df['EnergyFront'].map(lambda x: np.where(x < enThreshold, 0., x))\n",
    "    spacal_df['EnergyRear'] = spacal_df['EnergyRear'] .map(lambda x: np.where(x < enThreshold, 0., x))\n",
    "\n",
    "\n",
    "    #Get maximums and minimums in every cell:\n",
    "    maxEnergyFront = np.array([])\n",
    "    minEnergyFront = np.array([])\n",
    "    maxEnergyRear = np.array([])\n",
    "    minEnergyRear = np.array([])\n",
    "\n",
    "\n",
    "    for row in spacal_df.index:\n",
    "        maxEnergyFront = np.append(maxEnergyFront, np.amax(spacal_df[\"EnergyFront\"][row]))\n",
    "        minEnergyFront = np.append(minEnergyFront, np.amin(spacal_df[\"EnergyFront\"][row]))\n",
    "        maxEnergyRear = np.append(maxEnergyRear, np.amax(spacal_df[\"EnergyRear\"][row]))\n",
    "        minEnergyRear = np.append(minEnergyRear, np.amin(spacal_df[\"EnergyRear\"][row]))\n",
    "        \n",
    "\n",
    "    spacal_df[\"MaxEnergyFront\"] = maxEnergyFront\n",
    "    spacal_df[\"MinEnergyFront\"] = minEnergyFront\n",
    "    spacal_df[\"MaxEnergyRear\"] = maxEnergyRear\n",
    "    spacal_df[\"MinEnergyRear\"] = minEnergyRear\n",
    "\n",
    "\n",
    "    \n",
    "    #Once the data has been checked, it is possible to convert front and rear cells to one single image multichannel:\n",
    "    spacal_df[\"EnergyFrontRear\"] = list(map(lambda x,y: np.append(x,y), spacal_df[\"EnergyFront\"], spacal_df[\"EnergyRear\"]))\n",
    "\n",
    "\n",
    "    # we can put our network on our GPU. To do this, we can just set a flag like:\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda:0\")  # you can continue going on here, like cuda:1 cuda:2....etc. \n",
    "        print(\"Running on the GPU\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Running on the CPU\")\n",
    "    \n",
    "\n",
    "\n",
    "    ##############################\n",
    "    ###Section to record losses###\n",
    "    ##############################\n",
    "\n",
    "    #Add more utility to keep track of loss values and plot them at each epoch:\n",
    "    from IPython.display import clear_output\n",
    "\n",
    "    resultsDF = pd.DataFrame()\n",
    "\n",
    "\n",
    "    class Logger:\n",
    "      def __init__(self):\n",
    "        self.train_loss_batch = []\n",
    "        self.train_loss_epoch = []\n",
    "        self.test_loss_batch = []\n",
    "        self.test_loss_epoch = []\n",
    "        self.train_batches_per_epoch = 0\n",
    "        self.test_batches_per_epoch = 0\n",
    "        self.epoch_counter = 0\n",
    "\n",
    "      def fill_train(self, loss):\n",
    "        self.train_loss_batch.append(loss)\n",
    "        self.train_batches_per_epoch += 1\n",
    "\n",
    "      def fill_test(self, loss):\n",
    "        self.test_loss_batch.append(loss)\n",
    "        self.test_batches_per_epoch += 1\n",
    "\n",
    "      def finish_epoch(self):\n",
    "        self.train_loss_epoch.append(np.mean(\n",
    "            self.train_loss_batch[-self.train_batches_per_epoch:]\n",
    "        ))\n",
    "        self.test_loss_epoch.append(np.mean(\n",
    "            self.test_loss_batch[-self.test_batches_per_epoch:]\n",
    "        ))\n",
    "        self.train_batches_per_epoch = 0\n",
    "        self.test_batches_per_epoch = 0\n",
    "        \n",
    "        clear_output()\n",
    "      \n",
    "        print(\"epoch #{} \\t train_loss: {:.8} \\t test_loss: {:.8}\".format(\n",
    "                  self.epoch_counter,\n",
    "                  self.train_loss_epoch[-1],\n",
    "                  self.test_loss_epoch[-1]\n",
    "              ))\n",
    "        \n",
    "        self.epoch_counter += 1\n",
    "\n",
    "\n",
    "\n",
    "    #############################\n",
    "    ###Other functions section###\n",
    "    #############################\n",
    "\n",
    "\n",
    "    #This reshape class will be needed to reshape convolutions.\n",
    "    class Reshape(torch.nn.Module):\n",
    "        def __init__(self, *shape):\n",
    "            super(Reshape, self).__init__()\n",
    "            self.shape = shape\n",
    "\n",
    "        def forward(self, x):\n",
    "            return x.reshape(*self.shape)\n",
    "\n",
    "\n",
    "    #Convert train and test data to tensors\n",
    "    def setData(X_train, X_test, y_train, y_test, maxEnergyFrontRear, originalLen):\n",
    "        \n",
    "        X_train, X_test = X_train/maxEnergyFrontRear, X_test/maxEnergyFrontRear\n",
    "\n",
    "        try:\n",
    "            #Case 1: index 0 is in X_train\n",
    "            X_train = torch.tensor(X_train, dtype=torch.float32, requires_grad=True) \n",
    "            newIndex = []\n",
    "            for i in range(originalLen):\n",
    "                try:\n",
    "                    newIndex.append(X_test[i])\n",
    "                except:\n",
    "                    pass\n",
    "            X_test = torch.tensor(newIndex, dtype=torch.float32, requires_grad=True)\n",
    "        except:\n",
    "            #Case 2: index 0 is in X_test\n",
    "            X_test = torch.tensor(X_test,  dtype=torch.float32, requires_grad=True) \n",
    "            newIndex = []\n",
    "            for i in range(originalLen):\n",
    "                try:\n",
    "                    newIndex.append(X_train[i])\n",
    "                except:\n",
    "                    pass\n",
    "            X_train = torch.tensor(newIndex, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "        try:\n",
    "            #Case 1: index 0 is in y_train\n",
    "            y_train = torch.tensor(y_train, dtype=torch.long) \n",
    "            newIndex = []\n",
    "            for i in range(originalLen):\n",
    "                try:\n",
    "                    newIndex.append(y_test[i])\n",
    "                except:\n",
    "                    pass\n",
    "            y_test = torch.tensor(newIndex, dtype=torch.long)\n",
    "        except:\n",
    "            #Case 2: index 0 is in X_test\n",
    "            y_test = torch.tensor(y_test,  dtype=torch.long) \n",
    "            newIndex = []\n",
    "            for i in range(originalLen):\n",
    "                try:\n",
    "                    newIndex.append(y_train[i])\n",
    "                except:\n",
    "                    pass\n",
    "            y_train = torch.tensor(newIndex, dtype=torch.long)\n",
    "\n",
    "\n",
    "        return X_train, X_test, y_train, y_test   \n",
    "    \n",
    "\n",
    "    #Optimizer function. You can add as many as you want.\n",
    "    def getOptimizer(i, parameters, lr):\n",
    "        '''\n",
    "        requieres torch\n",
    "        '''\n",
    "        if(i == 0):\n",
    "            return torch.optim.Adam(parameters, lr)\n",
    "        if(i == 1):\n",
    "            return torch.optim.SGD(parameters, lr)\n",
    "        if(i == 2):\n",
    "            return torch.optim.Adamax(parameters, lr)\n",
    "\n",
    "    #Loss functions. You can add as many as you want.\n",
    "    def getLoss(i):\n",
    "        '''\n",
    "        requires torch\n",
    "        inspired by: https://medium.com/udacity-pytorch-challengers/a-brief-overview-of-loss-functions-in-pytorch-c0ddb78068f7\n",
    "        '''\n",
    "        if(i == 0):\n",
    "            return torch.nn.CrossEntropyLoss() \n",
    "        if(i == 1):\n",
    "            return torch.nn.NLLLoss() \n",
    "\n",
    "    #In other versions I would use this function to call a confusion matrix, here it just returns the accuracy and loss.\n",
    "    def getAccuracy(model, X_test, y_test, device, logger, loss_function, showCnf):\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        X_test = X_test.to(device)\n",
    "        y_test = y_test.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(X_test)\n",
    "            loss = loss_function(output, y_test)\n",
    "            logger.fill_test(loss.item())\n",
    "            for idx, i in enumerate(output):\n",
    "                if torch.argmax(i) == y_test[idx]:\n",
    "                    correct += 1\n",
    "                total += 1\n",
    "\n",
    "            #if(showCnf): printConfusionMatrix(output, y_test)#Uncommenting this line would generate an error due to lack of code.\n",
    "\n",
    "        return round(correct/total, 3)*100, logger\n",
    "\n",
    "\n",
    "    #This is the function with which the model is trained. It is prepared to work with different batch sizes and epochs that worked in other versions.\n",
    "    def trainCNN(spacal_df, KFold, device, model, learning_rate, BATCH_SIZE_Factor, EPOCHS, loss_function, optimizer):\n",
    "        '''\n",
    "        Requires setData.\n",
    "        Requires getLoss.\n",
    "        Requires getOptimizer.\n",
    "        '''\n",
    "\n",
    "        HeightRefFront = len(spacal_df.iloc[0,2]) # Height reference value in the front cells.\n",
    "        LongRefFront = len(spacal_df.iloc[0,2][0]) # Lognitude reference value in the front cells.\n",
    "\n",
    "        # modified (original)\n",
    "        # start = timer()       \n",
    "        \n",
    "        originalLen = len(spacal_df[\"EnergyFrontRear\"])\n",
    "\n",
    "        maxEnergyFrontRear = max(spacal_df[\"MaxEnergyRear\"].max(),spacal_df[\"MaxEnergyFront\"].max()) #Get the max value for both channels\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(spacal_df[\"EnergyFrontRear\"], spacal_df[\"class\"], test_size=(1/KFold))     \n",
    "        X_train, X_test, y_train, y_test = setData(X_train, X_test, y_train, y_test, maxEnergyFrontRear, originalLen)\n",
    "\n",
    "        #ToDo: Here you can modify the code to test with different batches and batch size.\n",
    "        #ToDo: Here you can modify the code to test with different Cross-Validation techniques.\n",
    "                \n",
    "\n",
    "        logger = Logger()\n",
    "        loss_function = getLoss(loss_function)\n",
    "        optimizer = getOptimizer(optimizer, model.parameters(), learning_rate)\n",
    "        \n",
    "        for i_epoch in range(EPOCHS):\n",
    "            # modified (new)\n",
    "            start = timer()       \n",
    "\n",
    "            model.zero_grad()  \n",
    "            output = model(X_train.to(device)) # pass the reshaped batch\n",
    "            loss = loss_function(output, y_train.to(device)) \n",
    "            logger.fill_train(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step() \n",
    "            \n",
    "            showCnf = True if i_epoch + 1 == EPOCHS else False\n",
    "            accuracyPctg, logger = getAccuracy(model, X_test, y_test, device, logger, loss_function, showCnf) #It will show cnfMatrix just in the last epoch.\n",
    "            logger.finish_epoch()\n",
    "\n",
    "            # modified (new)\n",
    "            #Return the model, the time, the accuracy and the loss of every epoch\n",
    "            time = round(timer() - start) \n",
    "            save2mlflow(logger.train_loss_epoch[-1], logger.test_loss_epoch[-1], time, accuracyPctg, device, model=model)\n",
    "            save_summary(model, X_test[0])\n",
    "            yield time, accuracyPctg, logger.test_loss_epoch[-1], logger.train_loss_epoch[-1] \n",
    "\n",
    "\n",
    "        # modified (original)\n",
    "        # #Return the model, the time, the accuracy and the loss of every epoch\n",
    "        # return time, accuracyPctg, logger.test_loss_epoch, logger.train_loss_epoch \n",
    "\n",
    "\n",
    "    def save2mlflow(train_loss, test_loss, time, accuracyPctg, enThreshold = enThreshold, KFold = KFold, model = model, variant = variant, learning_rate = learning_rate, BATCH_SIZE_Factor = BATCH_SIZE_Factor, EPOCHS = EPOCHS, loss_function = loss_function, optimizer = optimizer):\n",
    "        mlflow.log_param(\"enThreshold\", enThreshold)\n",
    "        mlflow.log_param(\"KFold\", KFold)\n",
    " \n",
    "        # modified (original), not sure this does anything\n",
    "        mlflow.log_param(\"model\", model)\n",
    "\n",
    "        mlflow.log_param(\"variant\", variant)\n",
    "        mlflow.log_param(\"learning_rate\", learning_rate)\n",
    "        mlflow.log_param(\"BATCH_SIZE_Factor\", BATCH_SIZE_Factor)\n",
    "        mlflow.log_param(\"EPOCHS\", EPOCHS)\n",
    "        mlflow.log_param(\"loss_function\", loss_function)\n",
    "        mlflow.log_param(\"optimizer\", optimizer)\n",
    "        \n",
    "        mlflow.log_metric(\"time\", time)\n",
    "        mlflow.log_metric(\"accuracyPctg\", accuracyPctg)\n",
    "        # modified (new)\n",
    "        mlflow.log_metric('Train loss', train_loss)\n",
    "        mlflow.log_metric('Test loss', test_loss)\n",
    "        # modified (new)\n",
    "        # save the model weights as a mlflow artifact. this function (now) gets called every epoch, but i like this because we save \"checkpoints\" every epoch\n",
    "        # that way we do not lose our progress if we stop training in the middle\n",
    "        # loading them again is a bit annoying, but once you get the hang of it it isnt bad\n",
    "        torch.save(model.state_dict(), 'artifacts/model_checkpoint.pyt')\n",
    "        mlflow.log_artifact('artifacts/model_checkpoint.pyt')\n",
    "        \n",
    "#         tracking_url_type_store = urlparse(mlflow.get_tracking_uri()).scheme\n",
    "\n",
    "#         # Model registry does not work with file store\n",
    "#         if tracking_url_type_store != \"file\":\n",
    "\n",
    "#             # Register the model\n",
    "#             # There are other ways to use the Model Registry, which depends on the use case,\n",
    "#             # please refer to the doc for more information:\n",
    "#             # https://mlflow.org/docs/latest/model-registry.html#api-workflow\n",
    "#             mlflow.sklearn.log_model(lr, \"model\", registered_model_name=\"2ParticlesCNN\")\n",
    "#         else:\n",
    "#             mlflow.sklearn.log_model(lr, \"model\")\n",
    "\n",
    "    # modified (new)\n",
    "    # save a diagram of the architecture\n",
    "    def save_summary(model, sample_input):\n",
    "        # this part saves the printed output of summary() to a text file\n",
    "        orig_stdout = sys.stdout\n",
    "        f = open('artifacts/model_summary.txt', 'w')\n",
    "        sys.stdout = f\n",
    "        summary(model, sample_input.shape)\n",
    "        sys.stdout = orig_stdout\n",
    "        f.close()\n",
    "        mlflow.log_artifact('artifacts/model_summary.txt')\n",
    "\n",
    "\n",
    "    def combinationsCNN(spacal_df, KFold, device, model, variant, learning_rate, dropout_rate, BATCH_SIZE_Factor, EPOCHS, loss_function, optimizer):\n",
    "\n",
    "        model = modelSelector(device, model, variant)\n",
    "\n",
    "        # modified (changed \"return\" to \"yield\")\n",
    "        return trainCNN(spacal_df, KFold, device, model, learning_rate, BATCH_SIZE_Factor, EPOCHS, loss_function, optimizer)\n",
    "\n",
    "\n",
    "    mlflow.set_experiment('Two particle classifier')\n",
    "    with mlflow.start_run():\n",
    "        # modified (original)\n",
    "        # combinationsResult = combinationsCNN(spacal_df, KFold, device, model, variant, learning_rate, \n",
    "        #                                      '', BATCH_SIZE_Factor, EPOCHS, loss_function, optimizer)\n",
    "        \n",
    "        # #Save the results into mlflow.    \n",
    "        # save2mlflow(combinationsResult[0], combinationsResult[1], device)\n",
    "\n",
    "        # modified (new)\n",
    "        for combinationsResult in combinationsCNN(spacal_df, KFold, device, model, variant, learning_rate, '', BATCH_SIZE_Factor, EPOCHS, loss_function, optimizer):\n",
    "            print('Epoch time: ', combinationsResult[0], ' Train Loss: ', combinationsResult[3], ' Test Loss: ', combinationsResult[2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "epoch #9 \t train_loss: -0.52571166 \t test_loss: -0.53409362\nEpoch time:  0  Train Loss:  -0.5257116556167603  Test Loss:  -0.5340936183929443\n"
    }
   ],
   "source": [
    "train(0, 5, 0, 0, 0.01, 0, 10, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}